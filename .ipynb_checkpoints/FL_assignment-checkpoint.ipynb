{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from typing import NamedTuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "# %load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# import gym\n",
    "\n",
    "# Define the parameters using NamedTuple\n",
    "class FrozenLakeParams(NamedTuple):\n",
    "    total_episodes: int  # Total episodes\n",
    "    learning_rate: float  # Learning rate\n",
    "    gamma: float  # Discounting rate\n",
    "    epsilon: float  # Exploration probability\n",
    "    map_size: int  # Number of tiles of one side of the squared environment\n",
    "    seed: int  # Define a seed so that we get reproducible results\n",
    "    is_slippery: bool  # If true the player will move in intended direction with probability of 1/3 else will move in either perpendicular direction with equal probability of 1/3 in both directions\n",
    "    n_runs: int  # Number of runs\n",
    "    action_size: int  # Number of possible actions\n",
    "    state_size: int  # Number of possible states\n",
    "    proba_frozen: float  # Probability that a tile is frozen\n",
    "    savefig_folder: Path  # Root folder where plots are saved\n",
    "\n",
    "\n",
    "params = FrozenLakeParams(\n",
    "    total_episodes=2000,\n",
    "    learning_rate=0.8,\n",
    "    gamma=0.95,\n",
    "    epsilon=0.1,\n",
    "    map_size=5,\n",
    "    seed=123,\n",
    "    is_slippery=False,\n",
    "    n_runs=20,\n",
    "    action_size=None,\n",
    "    state_size=None,\n",
    "    proba_frozen=0.9,\n",
    "    savefig_folder=Path(\"./img/\"),  \n",
    ")\n",
    "params\n",
    "\n",
    "# Set the seed\n",
    "rng = np.random.default_rng(params.seed)\n",
    "\n",
    "# Create the figure folder if it doesn't exist\n",
    "params.savefig_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frozen Lake env\n",
    "\n",
    "# # Create the Frozen Lake environment\n",
    "# env = gym.make(\n",
    "#     \"FrozenLake-v1\",\n",
    "#     desc=None,  # The description of the map will be generated automatically\n",
    "#     map_name=None,\n",
    "#     is_slippery=params.is_slippery,\n",
    "#     render_mode=\"human\",  # Change to \"rgb_array\" if you want to visualize the environment\n",
    "# )\n",
    "\n",
    "# Update the params object with action_size and state_size\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=params.is_slippery)\n",
    "params = params._replace(action_size=env.action_space.n)\n",
    "params = params._replace(state_size=env.observation_space.n)\n",
    "env.close()  # Don't forget to close the environment after getting the sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Q-Table\n",
    "\n",
    "class QLearningTable:\n",
    "    def __init__(self, gamma, learning_rate, state_size, action_size):\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.q_table = np.zeros((state_size, action_size))\n",
    "\n",
    "    def reset_qtable(self):\n",
    "        self.q_table = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "    def update(self, state, action, reward, new_state):\n",
    "        predict = self.q_table[state, action]\n",
    "        target = reward + self.gamma * np.max(self.q_table[new_state, :])\n",
    "        self.q_table[state, action] += self.learning_rate * (target - predict)\n",
    "\n",
    "\n",
    "# class EpsilonGreedyPolicy:\n",
    "#     def __init__(self, epsilon, rng=None):\n",
    "#         self.epsilon = epsilon\n",
    "#         self.rng = rng\n",
    "\n",
    "#     def choose_action(self, action_space, state, q_table):\n",
    "#         \"\"\"Choose an action `a` in the current world state (s).\"\"\"\n",
    "#         if self.rng is not None:\n",
    "#             # First, we randomize a number\n",
    "#             explor_exploit_tradeoff = self.rng.uniform(0, 1)\n",
    "\n",
    "#             # Exploration\n",
    "#             if explor_exploit_tradeoff < self.epsilon:\n",
    "#                 action = action_space.sample()\n",
    "\n",
    "#             # Exploitation (taking the biggest Q-value for this state)\n",
    "#             else:\n",
    "#                 # Break ties randomly\n",
    "#                 # If all actions are the same for this state, we choose a random one\n",
    "#                 # (otherwise `np.argmax()` would always take the first one)\n",
    "#                 if np.all(q_table[state, :] == q_table[state, 0]):\n",
    "#                     action = action_space.sample()\n",
    "#                 else:\n",
    "#                     action = np.argmax(q_table[state, :])\n",
    "#         else:\n",
    "#             # Original implementation without RNG\n",
    "#             if np.random.rand() < self.epsilon:\n",
    "#                 action = action_space.sample()\n",
    "#             else:\n",
    "#                 max_q_value = np.max(q_table[state, :])\n",
    "#                 action = np.random.choice(np.where(q_table[state, :] == max_q_value)[0])\n",
    "#         return action\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Modify the EpsilonGreedyPolicy class to use the random number generator\n",
    "# class EpsilonGreedyPolicy:\n",
    "#     def __init__(self, epsilon, rng):\n",
    "#         self.epsilon = epsilon\n",
    "#         self.rng = rng\n",
    "\n",
    "#     def choose_action(self, action_space, state, q_table):\n",
    "#         if self.rng.random() < self.epsilon:\n",
    "#             action = action_space.sample()\n",
    "#         else:\n",
    "#             max_q_value = np.max(q_table[state, :])\n",
    "#             argmax_actions = np.where(q_table[state, :] == max_q_value)[0]\n",
    "#             action = self.rng.choice(argmax_actions)\n",
    "#         return action\n",
    "\n",
    "\n",
    "\n",
    "class EpsilonGreedyPolicy:\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def choose_action(self, action_space, state, q_table):\n",
    "        \"\"\"Choose an action `a` in the current world state (s).\"\"\"\n",
    "        # First, we randomize a number\n",
    "        explor_exploit_tradeoff = rng.uniform(0, 1)\n",
    "\n",
    "        # Exploration\n",
    "        if explor_exploit_tradeoff < self.epsilon:\n",
    "            action = action_space.sample()\n",
    "\n",
    "        # Exploitation (taking the biggest Q-value for this state)\n",
    "        else:\n",
    "            # Break ties randomly\n",
    "            # If all actions are the same for this state, we choose a random one\n",
    "            # (otherwise `np.argmax()` would always take the first one)\n",
    "            if np.all(q_table[state, :] == q_table[state, 0]):\n",
    "                action = action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(q_table[state, :])\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner = Qlearning(\n",
    "#     learning_rate=params.learning_rate,\n",
    "#     gamma=params.gamma,\n",
    "#     state_size=params.state_size,\n",
    "#     action_size=params.action_size,\n",
    "# )\n",
    "# explorer = EpsilonGreedy(\n",
    "#     epsilon=params.epsilon,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env(params):\n",
    "    env = gym.make(\"FrozenLake-v1\", is_slippery=params.is_slippery)\n",
    "    learner = QLearningTable(\n",
    "        gamma=params.gamma,\n",
    "        learning_rate=params.learning_rate,\n",
    "        state_size=params.state_size,\n",
    "        action_size=params.action_size,\n",
    "    )\n",
    "    explorer = EpsilonGreedyPolicy(params.epsilon, rng=np.random.default_rng(params.seed))\n",
    "\n",
    "    rewards = np.zeros((params.total_episodes, params.n_runs))\n",
    "    steps = np.zeros((params.total_episodes, params.n_runs))\n",
    "    episodes = np.arange(params.total_episodes)\n",
    "    qtables = np.zeros((params.n_runs, params.state_size, params.action_size))\n",
    "    all_states = []\n",
    "    all_actions = []\n",
    "\n",
    "    for run in range(params.n_runs):  # Run several times to account for stochasticity\n",
    "        learner.reset_qtable()  # Reset the Q-table between runs\n",
    "\n",
    "        for episode in tqdm(episodes, desc=f\"Run {run+1}/{params.n_runs} - Episodes\", leave=False):\n",
    "            state = env.reset()  # Reset the environment\n",
    "            step = 0\n",
    "            done = False\n",
    "            total_rewards = 0\n",
    "\n",
    "            while not done:\n",
    "                action = explorer.choose_action(action_space=env.action_space, state=state, q_table=learner.q_table)\n",
    "\n",
    "                # Log all states and actions\n",
    "                all_states.append(state.item())  # Convert state to a scalar integer\n",
    "                all_actions.append(action)\n",
    "\n",
    "                # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                learner.update(state.item(), action, reward, new_state.item())\n",
    "\n",
    "                total_rewards += reward\n",
    "                step += 1\n",
    "\n",
    "                # Our new state is state\n",
    "                state = new_state\n",
    "\n",
    "            # Log all rewards and steps\n",
    "            rewards[episode, run] = total_rewards\n",
    "            steps[episode, run] = step\n",
    "\n",
    "        qtables[run, :, :] = learner.q_table\n",
    "\n",
    "    env.close()  # Close the environment when done\n",
    "    return rewards, steps, episodes, qtables, all_states, all_actions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "def postprocess(episodes, params, rewards, steps, map_size):\n",
    "    \"\"\"Convert the results of the simulation in dataframes.\"\"\"\n",
    "    res = pd.DataFrame(\n",
    "        data={\n",
    "            \"Episodes\": np.tile(episodes, reps=params.n_runs),\n",
    "            \"Rewards\": rewards.flatten(),\n",
    "            \"Steps\": steps.flatten(),\n",
    "        }\n",
    "    )\n",
    "    res[\"cum_rewards\"] = rewards.cumsum(axis=0).flatten(order=\"F\")\n",
    "    res[\"map_size\"] = np.repeat(f\"{map_size}x{map_size}\", res.shape[0])\n",
    "\n",
    "    st = pd.DataFrame(data={\"Episodes\": episodes, \"Steps\": steps.mean(axis=1)})\n",
    "    st[\"map_size\"] = np.repeat(f\"{map_size}x{map_size}\", st.shape[0])\n",
    "    return res, st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtable_directions_map(qtable, map_size):\n",
    "    \"\"\"Get the best learned action & map it to arrows.\"\"\"\n",
    "    qtable_val_max = qtable.max(axis=1).reshape(map_size, map_size)\n",
    "    qtable_best_action = np.argmax(qtable, axis=1).reshape(map_size, map_size)\n",
    "    directions = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n",
    "    qtable_directions = np.empty(qtable_best_action.flatten().shape, dtype=str)\n",
    "    eps = np.finfo(float).eps  # Minimum float number on the machine\n",
    "    for idx, val in enumerate(qtable_best_action.flatten()):\n",
    "        if qtable_val_max.flatten()[idx] > eps:\n",
    "            # Assign an arrow only if a minimal Q-value has been learned as best action\n",
    "            # otherwise since 0 is a direction, it also gets mapped on the tiles where\n",
    "            # it didn't actually learn anything\n",
    "            qtable_directions[idx] = directions[val]\n",
    "    qtable_directions = qtable_directions.reshape(map_size, map_size)\n",
    "    return qtable_val_max, qtable_directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_q_values_map(qtable, env, map_size):\n",
    "    \"\"\"Plot the last frame of the simulation and the policy learned.\"\"\"\n",
    "    qtable_val_max, qtable_directions = qtable_directions_map(qtable, map_size)\n",
    "\n",
    "    # Plot the last frame\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    ax[0].imshow(env.render())\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[0].set_title(\"Last frame\")\n",
    "\n",
    "    # Plot the policy\n",
    "    sns.heatmap(\n",
    "        qtable_val_max,\n",
    "        annot=qtable_directions,\n",
    "        fmt=\"\",\n",
    "        ax=ax[1],\n",
    "        cmap=sns.color_palette(\"Blues\", as_cmap=True),\n",
    "        linewidths=0.7,\n",
    "        linecolor=\"black\",\n",
    "        xticklabels=[],\n",
    "        yticklabels=[],\n",
    "        annot_kws={\"fontsize\": \"xx-large\"},\n",
    "    ).set(title=\"Learned Q-values\\nArrows represent best action\")\n",
    "    for _, spine in ax[1].spines.items():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(0.7)\n",
    "        spine.set_color(\"black\")\n",
    "    img_title = f\"frozenlake_q_values_{map_size}x{map_size}.png\"\n",
    "    fig.savefig(params.savefig_folder / img_title, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_states_actions_distribution(states, actions, map_size):\n",
    "    \"\"\"Plot the distributions of states and actions.\"\"\"\n",
    "    labels = {\"LEFT\": 0, \"DOWN\": 1, \"RIGHT\": 2, \"UP\": 3}\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    sns.histplot(data=states, ax=ax[0], kde=True)\n",
    "    ax[0].set_title(\"States\")\n",
    "    sns.histplot(data=actions, ax=ax[1])\n",
    "    ax[1].set_xticks(list(labels.values()), labels=labels.keys())\n",
    "    ax[1].set_title(\"Actions\")\n",
    "    fig.tight_layout()\n",
    "    img_title = f\"frozenlake_states_actions_distrib_{map_size}x{map_size}.png\"\n",
    "    fig.savefig(params.savefig_folder / img_title, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "EpsilonGreedyPolicy.__init__() got an unexpected keyword argument 'rng'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m\n\u001b[1;32m     17\u001b[0m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mseed(\n\u001b[1;32m     18\u001b[0m     params\u001b[38;5;241m.\u001b[39mseed\n\u001b[1;32m     19\u001b[0m )  \u001b[38;5;66;03m# Set the seed to get reproducible results when sampling the action space\u001b[39;00m\n\u001b[1;32m     20\u001b[0m learner \u001b[38;5;241m=\u001b[39m QLearningTable(\n\u001b[1;32m     21\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mlearning_rate,\n\u001b[1;32m     22\u001b[0m     gamma\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mgamma,\n\u001b[1;32m     23\u001b[0m     state_size\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mstate_size,\n\u001b[1;32m     24\u001b[0m     action_size\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39maction_size,\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m explorer \u001b[38;5;241m=\u001b[39m \u001b[43mEpsilonGreedyPolicy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_rng\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Provide the rng argument\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmap_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmap_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m rewards, steps, episodes, qtables, all_states, all_actions \u001b[38;5;241m=\u001b[39m run_env(params)\n",
      "\u001b[0;31mTypeError\u001b[0m: EpsilonGreedyPolicy.__init__() got an unexpected keyword argument 'rng'"
     ]
    }
   ],
   "source": [
    "map_sizes = [5, 8, 11, 14]\n",
    "res_all = pd.DataFrame()\n",
    "st_all = pd.DataFrame()\n",
    "\n",
    "for map_size in map_sizes:\n",
    "    env = gym.make(\n",
    "        \"FrozenLake-v1\",\n",
    "        is_slippery=params.is_slippery,\n",
    "        render_mode=\"rgb_array\",\n",
    "        desc=generate_random_map(\n",
    "            size=map_size, p=params.proba_frozen, seed=params.seed\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    params = params._replace(action_size=env.action_space.n)\n",
    "    params = params._replace(state_size=env.observation_space.n)\n",
    "    env.action_space.seed(\n",
    "        params.seed\n",
    "    )  # Set the seed to get reproducible results when sampling the action space\n",
    "    learner = QLearningTable(\n",
    "        learning_rate=params.learning_rate,\n",
    "        gamma=params.gamma,\n",
    "        state_size=params.state_size,\n",
    "        action_size=params.action_size,\n",
    "    )\n",
    "    explorer = EpsilonGreedyPolicy(\n",
    "        epsilon=params.epsilon,\n",
    "        rng=np.random.default_rng(params.seed)  # Provide the rng argument\n",
    "    )\n",
    "\n",
    "    print(f\"Map size: {map_size}x{map_size}\")\n",
    "    rewards, steps, episodes, qtables, all_states, all_actions = run_env(params)\n",
    "\n",
    "    # Save the results in dataframes\n",
    "    res, st = postprocess(episodes, params, rewards, steps, map_size)\n",
    "    res_all = pd.concat([res_all, res])\n",
    "    st_all = pd.concat([st_all, st])\n",
    "    qtable = qtables.mean(axis=0)  # Average the Q-table between runs\n",
    "\n",
    "    plot_states_actions_distribution(\n",
    "        states=all_states, actions=all_actions, map_size=map_size\n",
    "    )  # Sanity check\n",
    "    plot_q_values_map(qtable, env, map_size)\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_steps_and_rewards(rewards_df, steps_df):\n",
    "    \"\"\"Plot the steps and rewards from dataframes.\"\"\"\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    sns.lineplot(\n",
    "        data=rewards_df, x=\"Episodes\", y=\"cum_rewards\", hue=\"map_size\", ax=ax[0]\n",
    "    )\n",
    "    ax[0].set(ylabel=\"Cumulated rewards\")\n",
    "\n",
    "    sns.lineplot(data=steps_df, x=\"Episodes\", y=\"Steps\", hue=\"map_size\", ax=ax[1])\n",
    "    ax[1].set(ylabel=\"Averaged steps number\")\n",
    "\n",
    "    for axi in ax:\n",
    "        axi.legend(title=\"map size\")\n",
    "    fig.tight_layout()\n",
    "    img_title = \"frozenlake_steps_and_rewards.png\"\n",
    "    fig.savefig(params.savefig_folder / img_title, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_steps_and_rewards(res_all, st_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
