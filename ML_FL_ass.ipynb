{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from typing import NamedTuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "\n",
    "sns.set_theme()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Define the parameters using NamedTuple\n",
    "class FrozenLakeParams(NamedTuple):\n",
    "    total_episodes: int\n",
    "    learning_rate: float\n",
    "    gamma: float\n",
    "    epsilon: float\n",
    "    map_size: int\n",
    "    seed: int\n",
    "    is_slippery: bool\n",
    "    n_runs: int\n",
    "    action_size: int\n",
    "    state_size: int\n",
    "    proba_frozen: float\n",
    "    savefig_folder: Path\n",
    "    \n",
    "# Create the FrozenLakeParams instance \n",
    "params = FrozenLakeParams(\n",
    "    total_episodes=2000,\n",
    "    learning_rate=0.8,\n",
    "    gamma=0.95,\n",
    "    epsilon=0.1,\n",
    "    map_size=5,\n",
    "    seed=123,\n",
    "    is_slippery=False,\n",
    "    n_runs=20,\n",
    "    action_size=None,\n",
    "    state_size=None,\n",
    "    proba_frozen=0.9,\n",
    "    savefig_folder=Path(\"./img/\"),\n",
    ") \n",
    "\n",
    "# Set the seed\n",
    "rng = np.random.default_rng(params.seed)\n",
    "\n",
    "# Create the figure folder if it doesn't exist\n",
    "params.savefig_folder.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frozen Lake env\n",
    "\n",
    "# Create the Frozen Lake environment\n",
    "env = gym.make(\n",
    "    \"FrozenLake-v1\",\n",
    "    desc=None,  # The description of the map will be generated automatically\n",
    "    map_name=None,\n",
    "    is_slippery=params.is_slippery,\n",
    "    render_mode=\"rgb_array\", \n",
    ")\n",
    "\n",
    "# Update the params object with the correct state_size and action_size\n",
    "params = params._replace(action_size=env.action_space.n)\n",
    "params = params._replace(state_size=env.observation_space.n)\n",
    "\n",
    "env.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Q-Table\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "class QLearningTable:\n",
    "    def __init__(self, gamma, learning_rate, state_size, action_size):\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.q_table = np.zeros((state_size, action_size))\n",
    "\n",
    "    def reset_qtable(self):\n",
    "        self.q_table = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "    def update(self, state, action, reward, new_state):\n",
    "        predict = self.q_table[state, action]\n",
    "        target = reward + self.gamma * np.max(self.q_table[new_state, :])\n",
    "        self.q_table[state, action] += self.learning_rate * (target - predict)\n",
    "\n",
    "class EpsilonGreedyPolicy:\n",
    "    def __init__(self, epsilon, rng):\n",
    "        self.epsilon = epsilon\n",
    "        self.rng = rng\n",
    "\n",
    "    def choose_action(self, action_space, state, q_table):\n",
    "        \"\"\"Choose an action `a` in the current world state (s).\"\"\"\n",
    "        # First, we randomize a number\n",
    "        explor_exploit_tradeoff = self.rng.uniform(0, 1)\n",
    "\n",
    "        # Exploration\n",
    "        if explor_exploit_tradeoff < self.epsilon:\n",
    "            action = action_space.sample()\n",
    "\n",
    "        # Exploitation (taking the biggest Q-value for this state)\n",
    "        else:\n",
    "            # Break ties randomly\n",
    "            # If all actions are the same for this state, we choose a random one\n",
    "            # (otherwise `np.argmax()` would always take the first one)\n",
    "            if np.all(q_table[state, :] == q_table[state, 0]):\n",
    "                action = action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(q_table[state, :])\n",
    "        return action\n",
    "\n",
    "\n",
    "rng = np.random.default_rng()  # an instance of the random number generator\n",
    "explorer = EpsilonGreedyPolicy(epsilon=params.epsilon, rng=rng)  # Pass the rng instance to the EpsilonGreedyPolicy class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner = Qlearning(\n",
    "#     learning_rate=params.learning_rate,\n",
    "#     gamma=params.gamma,\n",
    "#     state_size=params.state_size,\n",
    "#     action_size=params.action_size,\n",
    "# )\n",
    "# explorer = EpsilonGreedy(\n",
    "#     epsilon=params.epsilon,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env(params):\n",
    "    env = gym.make(\"FrozenLake-v1\", is_slippery=params.is_slippery)\n",
    "    learner = QLearningTable(\n",
    "        gamma=params.gamma,\n",
    "        learning_rate=params.learning_rate,\n",
    "        state_size=params.state_size,\n",
    "        action_size=params.action_size,\n",
    "    )\n",
    "    explorer = EpsilonGreedyPolicy(params.epsilon, rng=np.random.default_rng(params.seed))\n",
    "\n",
    "    rewards = np.zeros((params.total_episodes, params.n_runs))\n",
    "    steps = np.zeros((params.total_episodes, params.n_runs))\n",
    "    episodes = np.arange(params.total_episodes)\n",
    "    qtables = np.zeros((params.n_runs, params.state_size, params.action_size))\n",
    "    all_states = []\n",
    "    all_actions = []\n",
    "\n",
    "    for run in range(params.n_runs):  # Run several times to account for stochasticity\n",
    "        learner.reset_qtable()  # Reset the Q-table between runs\n",
    "\n",
    "        for episode in tqdm(episodes, desc=f\"Run {run+1}/{params.n_runs} - Episodes\", leave=False):\n",
    "            state = env.reset()  # state is a tuple, e.g., (0,)\n",
    "            state = state[0]  # Convert state to a scalar integer\n",
    "            step = 0\n",
    "            done = False\n",
    "            total_rewards = 0\n",
    "            \n",
    "            while not done:\n",
    "                action = explorer.choose_action(action_space=env.action_space, state=state, q_table=learner.q_table)\n",
    "\n",
    "            # Log all states and actions\n",
    "            all_states.append(state)  # Append the scalar state value\n",
    "            all_actions.append(action)\n",
    "\n",
    "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            new_state, reward, done, _, = env.step(action)  # Store only the required values\n",
    "\n",
    "            learner.update(state, action, reward, new_state[0])  # Convert new_state to scalar integer\n",
    "\n",
    "            total_rewards += reward\n",
    "            step += 1\n",
    "\n",
    "            # Our new state is state\n",
    "            state = new_state[0]  # Convert new_state to scalar integer\n",
    "\n",
    "\n",
    "            # Log all rewards and steps\n",
    "            rewards[episode, run] = total_rewards\n",
    "            steps[episode, run] = step\n",
    "\n",
    "        qtables[run, :, :] = learner.q_table\n",
    "\n",
    "    env.close() \n",
    "    return rewards, steps, episodes, qtables, all_states, all_actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 1/20 - Episodes:   0%|                                                             | 0/2000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Visualize the impact of learning_rate on average rewards and steps\n",
    "\n",
    "\n",
    "# plot average rewards and steps for different hyperparameter settings\n",
    "def plot_hyperparameter_analysis(results, hyperparameter, hyperparameter_values, ylabel):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    for i, value in enumerate(hyperparameter_values):\n",
    "        avg_results = np.mean(results[i], axis=1)\n",
    "        sns.lineplot(data=pd.Series(avg_results), ax=ax, label=f\"{hyperparameter} = {value}\")\n",
    "    ax.set_xlabel(\"Episodes\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Test different hyperparameter settings\n",
    "learning_rates = [0.2, 0.5, 0.8]\n",
    "gammas = [0.8, 0.9, 0.95]\n",
    "epsilons = [0.1, 0.2, 0.3]\n",
    "\n",
    "all_rewards = []\n",
    "all_steps = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for gamma in gammas:\n",
    "        for epsilon in epsilons:\n",
    "            # Update the params instance with the new hyperparameters\n",
    "            params = params._replace(learning_rate=lr, gamma=gamma, epsilon=epsilon)\n",
    "\n",
    "            rewards, steps, _, _, _, _ = run_env(params)\n",
    "            all_rewards.append(rewards)\n",
    "            all_steps.append(steps)\n",
    "\n",
    "\n",
    "# Plot the impact of learning_rate on average rewards and steps\n",
    "plot_hyperparameter_analysis(all_rewards, \"Learning Rate\", learning_rates, \"Average Rewards\")\n",
    "plot_hyperparameter_analysis(all_steps, \"Learning Rate\", learning_rates, \"Average Steps\")\n",
    "\n",
    "# Plot the impact of gamma on average rewards and steps\n",
    "plot_hyperparameter_analysis(all_rewards, \"Gamma\", gammas, \"Average Rewards\")\n",
    "plot_hyperparameter_analysis(all_steps, \"Gamma\", gammas, \"Average Steps\")\n",
    "\n",
    "# Plot the impact of epsilon on average rewards and steps\n",
    "plot_hyperparameter_analysis(all_rewards, \"Epsilon\", epsilons, \"Average Rewards\")\n",
    "plot_hyperparameter_analysis(all_steps, \"Epsilon\", epsilons, \"Average Steps\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "def postprocess(episodes, params, rewards, steps, map_size):\n",
    "    \"\"\"Convert the results of the simulation in dataframes.\"\"\"\n",
    "    res = pd.DataFrame(\n",
    "        data={\n",
    "            \"Episodes\": np.tile(episodes, reps=params.n_runs),\n",
    "            \"Rewards\": rewards.flatten(),\n",
    "            \"Steps\": steps.flatten(),\n",
    "        }\n",
    "    )\n",
    "    res[\"cum_rewards\"] = rewards.cumsum(axis=0).flatten(order=\"F\")\n",
    "    res[\"map_size\"] = np.repeat(f\"{map_size}x{map_size}\", res.shape[0])\n",
    "\n",
    "    st = pd.DataFrame(data={\"Episodes\": episodes, \"Steps\": steps.mean(axis=1)})\n",
    "    st[\"map_size\"] = np.repeat(f\"{map_size}x{map_size}\", st.shape[0])\n",
    "    return res, st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtable_directions_map(qtable, map_size):\n",
    "    \"\"\"Get the best learned action & map it to arrows.\"\"\"\n",
    "    qtable_val_max = qtable.max(axis=1).reshape(map_size, map_size)\n",
    "    qtable_best_action = np.argmax(qtable, axis=1).reshape(map_size, map_size)\n",
    "    directions = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n",
    "    qtable_directions = np.empty(qtable_best_action.flatten().shape, dtype=str)\n",
    "    eps = np.finfo(float).eps  # Minimum float number on the machine\n",
    "    for idx, val in enumerate(qtable_best_action.flatten()):\n",
    "        if qtable_val_max.flatten()[idx] > eps:\n",
    "            # Assign an arrow only if a minimal Q-value has been learned as best action\n",
    "            # otherwise since 0 is a direction, it also gets mapped on the tiles where\n",
    "            # it didn't actually learn anything\n",
    "            qtable_directions[idx] = directions[val]\n",
    "    qtable_directions = qtable_directions.reshape(map_size, map_size)\n",
    "    return qtable_val_max, qtable_directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_q_values_map(qtable, env, map_size):\n",
    "    \"\"\"Plot the last frame of the simulation and the policy learned.\"\"\"\n",
    "    qtable_val_max, qtable_directions = qtable_directions_map(qtable, map_size)\n",
    "\n",
    "    # Plot the last frame\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    ax[0].imshow(env.render())\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[0].set_title(\"Last frame\")\n",
    "\n",
    "    # Plot the policy\n",
    "    sns.heatmap(\n",
    "        qtable_val_max,\n",
    "        annot=qtable_directions,\n",
    "        fmt=\"\",\n",
    "        ax=ax[1],\n",
    "        cmap=sns.color_palette(\"Blues\", as_cmap=True),\n",
    "        linewidths=0.7,\n",
    "        linecolor=\"black\",\n",
    "        xticklabels=[],\n",
    "        yticklabels=[],\n",
    "        annot_kws={\"fontsize\": \"xx-large\"},\n",
    "    ).set(title=\"Learned Q-values\\nArrows represent best action\")\n",
    "    for _, spine in ax[1].spines.items():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(0.7)\n",
    "        spine.set_color(\"black\")\n",
    "    img_title = f\"frozenlake_q_values_{map_size}x{map_size}.png\"\n",
    "    fig.savefig(params.savefig_folder / img_title, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_states_actions_distribution(states, actions, map_size):\n",
    "    \"\"\"Plot the distributions of states and actions.\"\"\"\n",
    "    labels = {\"LEFT\": 0, \"DOWN\": 1, \"RIGHT\": 2, \"UP\": 3}\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    sns.histplot(data=states, ax=ax[0], kde=True)\n",
    "    ax[0].set_title(\"States\")\n",
    "    sns.histplot(data=actions, ax=ax[1])\n",
    "    ax[1].set_xticks(list(labels.values()), labels=labels.keys())\n",
    "    ax[1].set_title(\"Actions\")\n",
    "    fig.tight_layout()\n",
    "    img_title = f\"frozenlake_states_actions_distrib_{map_size}x{map_size}.png\"\n",
    "    fig.savefig(params.savefig_folder / img_title, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_sizes = [5, 8, 11, 14]\n",
    "res_all = pd.DataFrame()\n",
    "st_all = pd.DataFrame()\n",
    "\n",
    "for map_size in map_sizes:\n",
    "    env = gym.make(\n",
    "        \"FrozenLake-v1\",\n",
    "        is_slippery=params.is_slippery,\n",
    "        render_mode=\"rgb_array\",\n",
    "        desc=generate_random_map(\n",
    "            size=map_size, p=params.proba_frozen, seed=params.seed\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    params = params._replace(action_size=env.action_space.n)\n",
    "    params = params._replace(state_size=env.observation_space.n)\n",
    "    env.action_space.seed(\n",
    "        params.seed\n",
    "    )  # Set the seed to get reproducible results when sampling the action space\n",
    "    learner = QLearningTable(\n",
    "        learning_rate=params.learning_rate,\n",
    "        gamma=params.gamma,\n",
    "        state_size=params.state_size,\n",
    "        action_size=params.action_size,\n",
    "    )\n",
    "    explorer = EpsilonGreedyPolicy(\n",
    "        epsilon=params.epsilon,\n",
    "    )\n",
    "\n",
    "    print(f\"Map size: {map_size}x{map_size}\")\n",
    "    rewards, steps, episodes, qtables, all_states, all_actions = run_env()\n",
    "\n",
    "    # Save the results in dataframes\n",
    "    res, st = postprocess(episodes, params, rewards, steps, map_size)\n",
    "    res_all = pd.concat([res_all, res])\n",
    "    st_all = pd.concat([st_all, st])\n",
    "    qtable = qtables.mean(axis=0)  # Average the Q-table between runs\n",
    "\n",
    "    plot_states_actions_distribution(\n",
    "        states=all_states, actions=all_actions, map_size=map_size\n",
    "    )  # Sanity check\n",
    "    plot_q_values_map(qtable, env, map_size)\n",
    "\n",
    "    env.close()  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_steps_and_rewards(rewards_df, steps_df):\n",
    "    \"\"\"Plot the steps and rewards from dataframes.\"\"\"\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    sns.lineplot(\n",
    "        data=rewards_df, x=\"Episodes\", y=\"cum_rewards\", hue=\"map_size\", ax=ax[0]\n",
    "    )\n",
    "    ax[0].set(ylabel=\"Cumulated rewards\")\n",
    "\n",
    "    sns.lineplot(data=steps_df, x=\"Episodes\", y=\"Steps\", hue=\"map_size\", ax=ax[1])\n",
    "    ax[1].set(ylabel=\"Averaged steps number\")\n",
    "\n",
    "    for axi in ax:\n",
    "        axi.legend(title=\"map size\")\n",
    "    fig.tight_layout()\n",
    "    img_title = \"frozenlake_steps_and_rewards.png\"\n",
    "    fig.savefig(params.savefig_folder / img_title, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_steps_and_rewards(res_all, st_all)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
